# itjuzi
Crawling on IT Orange Subnet for information of founders.
<h1>
        第一步：登录——login.py
    </h1>
    <p>
        控制登录，分为密码登录和cookie登录两种方式，其中密码登录需要在info.json中填入正确的账号和密码，cookie登录必须使用密码登录成功登录一次之后才能使用；
    </p>
    <h1>
        第二步：确定初始页面，抓取总数据——classify_index.py/crawler_index.py/person.py
    </h1>
    <p>
        在桔子网的<a href="https://www.itjuzi.com/person/">人才库页面<a>，按照行业、国内外、融资轮次三种分类方式找到符合搜寻条件的创业者，将这些创业者的信息写入一个文本文件，每一个人占一行，每一项用'\t'分开，排列顺序是——姓名、项目/品牌/职位、个人简介、行业、融资轮次、国内外、个人页面的url，实现代码——classify_index.py和crawler_index.py，借助person.py的Person类实现抓取，存储结果：persons_source.txt（直接生成的结果）/persons_processed.txt（去除不必要的换行和空行）/count.txt（各个分类标准下的创业者数量统计）；
    </p>
    <h1>
        第三步：选样本——temp.py(deleted)
    </h1>
    <p>
        考虑到开发过程可能出现较多问题，按照1%的比例从总体的persons_processed.txt中随机抽取了样本，存储在person_sample.txt文件中，格式不变；
    </p>
    <h1>
        第四步：针对每一个创业者的页面进行解析——crawler.py/itjuzi_debug.py
    </h1>
    <p>
        通过crawler.py中Crawler类的process_person()方法实现信息抓取实现，传入一个个人页面的url，若可以连接，该方法将返回一个整数（创业项目数量）和一个数组（所在城市、工作经历、教育经历、项目名称、所处行业、公司页面url、公司成立时间、公司简介、图片链接）；如不能连接，该方法会报错，将这一情况在使用时用try-except进行处理，在调试时将数组进行了存储，存储文件为sample_person.txt，存储的实现代码为itjuzi_debug.py的person_to_company()函数，对于无法连接到url登记为f'{url}\t unable to access'，对于可以连接的url，对每个人的项目进行了排序，加上了f'No.{i}\t {创始人url}\t'，数组中教育经历之后的项目都用'///'进行分隔；
    </p>
    <h1>
        第五步：针对每一个公司的页面进行解析——crawler.py/itjuzi_debug.py
    </h1>
    <p>
        通过crawler.py中Crawler类的process_company()方法实现，传入公司页面的url，工作原理和上一步相似，返回的顺序是公司注册名、电话、邮件、项目简介、赛道关键词、最近一轮融资时间、最近一轮融资轮次、最近一轮融资额、参与该轮的投资机构、官方网站、通讯地址；
    </p>
    <p>
        由于该步骤各个公司区别较大，故使用了较多的try-except语句，此外，将无法获取的信息登记为'empty'，调试时对公司细节进行了存储，存储的实现代码是itjuzi_debug.py的company_details()函数，存储文件为sample_company.txt，在返回值之前添加了f'{公司的url}\t'，对于无效的人物（即错误的url，将登记为'nothing\t unable to access'）；
    </p>
    <h1>
        第六步：整合——itjuzi.py
    </h1>
    <p>
        从人才库页面抓取的创业人物数据出发，循环抓取，每个人循环的次数是其参与的项目数，考虑到某些数据在不同页面会有重复和冲突，设置数据的优先级——
        <ul>
            <li>姓名——初始页面</li>
            <li>产品/项目品牌 名称——个人页面->初始页面</li>
            <li>公司注册名——公司页面（工商信息栏->基本信息栏）</li>
            <li>手机号——公司页面（若不借助其他方法，只能找到公司的座机号）</li>
            <li>邮箱——公司页面（同手机号）</li>
            <li>个人主页——个人页面+初始页面+个人页面url（时间关系没有拓展其他渠道搜索信息了）</li>
            <li>项目简介——公司页面->个人页面（似乎是包含关系，数据相同）</li>
            <li>赛道关键词——公司页面的标签</li>
            <li>成立时间——个人页面</li>
            <li>当前轮次——公司页面融资数据->初始页面</li>
            <li>当前轮次融资金额——公司页面融资数据（总额也是很容易计算的）
            </li>
            <li>当前轮次投资机构——公司页面融资数据</li>
            <li>通讯地址——公司页面->创业者所在地</li>
            <li>创业者城市——个人页面</li>
            <li>公司官网——公司页面</li>
        </ul>
    </p>
    <p>
        先将这些文件写入文本文档，观察处理之后导入excel表格当中。
    </p>
    <h1>
        其他说明
    </h1>
    <li>
        为了更直观地爬取和尽量避免被封IP，采用了比较慢的selenium方法又因为网站要登陆，所以没法采取多线程的方法，导致时间稍显紧迫，所以很多细节没有优化，爬虫的效率不高，整个程序也是在debug中不断前进，所以肯定还存在一些bug，日后会不断改善。
    </li>
    <li>
        因为没有会员，每个分类只能看3页，所以在爬虫中尽量采取了更加细致的分类来获取更多的信息，但是抓取的数据量依然比较少（初创企业限制也是一个原因），只有4000+创业人物，不过，从count.txt的统计结果来看，这种方式抓取的数量达到了一半以上，不可否认有一定的效果。
    </li>
    <li>
      因为这个库是实习初期建立的，当时还不太习惯pandas/openpyxl等处理表格的库，所以很多用文本文件字符串的方法进行处理，显然用表格库来做效果会好得多。
    </li>
